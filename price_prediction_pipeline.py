# -*- coding: utf-8 -*-
"""price-prediction-pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eBk2LB8s_q1AENC8zbF8uE4Q3mfGeANK
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col,sum
spark = SparkSession.builder \
    .appName("avocado price pred") \
    .getOrCreate()
df = spark.read.csv('/kaggle/input/avocado-prices/avocado.csv', header=True, inferSchema=True)
df.show(5)

df.printSchema()

df.select([sum(col(c).isNull().cast("int")).alias(c) for c in df.columns]).show()
df.count()

df=df.drop('_c0')
df = df.withColumnRenamed("4046", "PLU_4046") \
       .withColumnRenamed("4225", "PLU_4225") \
       .withColumnRenamed("4770", "PLU_4770")

from pyspark.sql.functions import to_date,day,month,year,dayofweek

df = df.withColumn("Date", to_date("Date", "yyyy-MM-dd"))

df=df.withColumn('Day',day(col('Date')))\
     .withColumn('Month',month(col('Date')))\
      .withColumn('year',year(col('Date')))\
       .withColumn('Day_of_week',dayofweek(col('Date')))
df=df.drop('Date')

from pyspark.sql.functions import when

df = df.withColumn("PLU_4046_Ratio", col("PLU_4046") / col("Total Volume"))\
       .withColumn("PLU_4225_Ratio", col("PLU_4225") / col("Total Volume"))\
       .withColumn("PLU_4770_Ratio", col("PLU_4770") / col("Total Volume"))\
       .withColumn("SmallBag_Ratio", col("Small Bags") / col("Total Bags"))\
       .withColumn("LargeBag_Ratio", col("Large Bags") / col("Total Bags"))\
       .withColumn("XLargeBag_Ratio", col("XLarge Bags") / col("Total Bags"))

df = df.withColumn("Season",
    when(col("Month").isin(12, 1, 2), "Winter")
    .when(col("Month").isin(3, 4, 5), "Spring")
    .when(col("Month").isin(6, 7, 8), "Summer")
    .otherwise("Fall")
)

from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, MinMaxScaler, StringIndexer
from pyspark.ml.regression import RandomForestRegressor
df=df.dropna()
numeric_cols = [
    'Total Volume', 'PLU_4046', 'PLU_4225', 'PLU_4770',
    'Total Bags', 'Small Bags', 'Large Bags', 'XLarge Bags',
    'year', 'Day', 'Month', 'Day_of_week',
    'PLU_4046_Ratio', 'PLU_4225_Ratio', 'PLU_4770_Ratio',
    'SmallBag_Ratio', 'LargeBag_Ratio', 'XLargeBag_Ratio']

categorical_cols = ['type', 'region', 'Season']

indexers = [StringIndexer(inputCol=col, outputCol=col + '_encoded') for col in categorical_cols]
encoded_cols = [col + "_encoded" for col in categorical_cols]

assembler_input = numeric_cols + encoded_cols

assembler = VectorAssembler(inputCols=assembler_input, outputCol='assembled_features')

scaler = MinMaxScaler(inputCol='assembled_features', outputCol='features')

train_data, test_data = df.randomSplit([0.8, 0.2], seed=1)

rf = RandomForestRegressor(featuresCol='features', labelCol='AveragePrice', numTrees=50, maxDepth=10)

pipeline = Pipeline(stages=indexers + [assembler, scaler, rf])

model = pipeline.fit(train_data)

predictions = model.transform(test_data)

predictions.select('AveragePrice', 'prediction').show(10)

from pyspark.ml.regression import RandomForestRegressor

rmse_rf = evaluator_rmse.evaluate(predictions_rf)
mae_rf = evaluator_mae.evaluate(predictions_rf)
r2_rf = evaluator_r2.evaluate(predictions_rf)

print(f"RF RMSE: {rmse_rf:.4f}")
print(f"RF MAE: {mae_rf:.4f}")
print(f"RF R2: {r2_rf:.4f}")