{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":38613,"sourceType":"datasetVersion","datasetId":30292}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:32:01.333266Z","iopub.execute_input":"2025-07-31T20:32:01.333703Z","iopub.status.idle":"2025-07-31T20:32:01.343674Z","shell.execute_reply.started":"2025-07-31T20:32:01.333678Z","shell.execute_reply":"2025-07-31T20:32:01.342149Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/avocado-prices/avocado.csv\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum\nspark = SparkSession.builder \\\n    .appName(\"avocado price pred\") \\\n    .getOrCreate()\ndf = spark.read.csv('/kaggle/input/avocado-prices/avocado.csv', header=True, inferSchema=True)\ndf.show(5)\n\ndf.printSchema()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:32:01.345950Z","iopub.execute_input":"2025-07-31T20:32:01.346369Z","iopub.status.idle":"2025-07-31T20:32:01.725030Z","shell.execute_reply.started":"2025-07-31T20:32:01.346335Z","shell.execute_reply":"2025-07-31T20:32:01.723981Z"}},"outputs":[{"name":"stdout","text":"+---+----------+------------+------------+-------+---------+-----+----------+----------+----------+-----------+------------+----+------+\n|_c0|      Date|AveragePrice|Total Volume|   4046|     4225| 4770|Total Bags|Small Bags|Large Bags|XLarge Bags|        type|year|region|\n+---+----------+------------+------------+-------+---------+-----+----------+----------+----------+-----------+------------+----+------+\n|  0|2015-12-27|        1.33|    64236.62|1036.74| 54454.85|48.16|   8696.87|   8603.62|     93.25|        0.0|conventional|2015|Albany|\n|  1|2015-12-20|        1.35|    54876.98| 674.28| 44638.81|58.33|   9505.56|   9408.07|     97.49|        0.0|conventional|2015|Albany|\n|  2|2015-12-13|        0.93|   118220.22|  794.7|109149.67|130.5|   8145.35|   8042.21|    103.14|        0.0|conventional|2015|Albany|\n|  3|2015-12-06|        1.08|    78992.15| 1132.0| 71976.41|72.58|   5811.16|    5677.4|    133.76|        0.0|conventional|2015|Albany|\n|  4|2015-11-29|        1.28|     51039.6| 941.48| 43838.39|75.78|   6183.95|   5986.26|    197.69|        0.0|conventional|2015|Albany|\n+---+----------+------------+------------+-------+---------+-----+----------+----------+----------+-----------+------------+----+------+\nonly showing top 5 rows\n\nroot\n |-- _c0: integer (nullable = true)\n |-- Date: date (nullable = true)\n |-- AveragePrice: double (nullable = true)\n |-- Total Volume: double (nullable = true)\n |-- 4046: double (nullable = true)\n |-- 4225: double (nullable = true)\n |-- 4770: double (nullable = true)\n |-- Total Bags: double (nullable = true)\n |-- Small Bags: double (nullable = true)\n |-- Large Bags: double (nullable = true)\n |-- XLarge Bags: double (nullable = true)\n |-- type: string (nullable = true)\n |-- year: integer (nullable = true)\n |-- region: string (nullable = true)\n\n","output_type":"stream"},{"name":"stderr","text":"25/07/31 20:32:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , Date, AveragePrice, Total Volume, 4046, 4225, 4770, Total Bags, Small Bags, Large Bags, XLarge Bags, type, year, region\n Schema: _c0, Date, AveragePrice, Total Volume, 4046, 4225, 4770, Total Bags, Small Bags, Large Bags, XLarge Bags, type, year, region\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/avocado-prices/avocado.csv\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()\ndf.count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:32:01.725909Z","iopub.execute_input":"2025-07-31T20:32:01.726161Z","iopub.status.idle":"2025-07-31T20:32:02.179905Z","shell.execute_reply.started":"2025-07-31T20:32:01.726141Z","shell.execute_reply":"2025-07-31T20:32:02.178204Z"}},"outputs":[{"name":"stderr","text":"25/07/31 20:32:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , Date, AveragePrice, Total Volume, 4046, 4225, 4770, Total Bags, Small Bags, Large Bags, XLarge Bags, type, year, region\n Schema: _c0, Date, AveragePrice, Total Volume, 4046, 4225, 4770, Total Bags, Small Bags, Large Bags, XLarge Bags, type, year, region\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/avocado-prices/avocado.csv\n","output_type":"stream"},{"name":"stdout","text":"+---+----+------------+------------+----+----+----+----------+----------+----------+-----------+----+----+------+\n|_c0|Date|AveragePrice|Total Volume|4046|4225|4770|Total Bags|Small Bags|Large Bags|XLarge Bags|type|year|region|\n+---+----+------------+------------+----+----+----+----------+----------+----------+-----------+----+----+------+\n|  0|   0|           0|           0|   0|   0|   0|         0|         0|         0|          0|   0|   0|     0|\n+---+----+------------+------------+----+----+----+----------+----------+----------+-----------+----+----+------+\n\n","output_type":"stream"},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"18249"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"df=df.drop('_c0')\ndf = df.withColumnRenamed(\"4046\", \"PLU_4046\") \\\n       .withColumnRenamed(\"4225\", \"PLU_4225\") \\\n       .withColumnRenamed(\"4770\", \"PLU_4770\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:32:02.182200Z","iopub.execute_input":"2025-07-31T20:32:02.183043Z","iopub.status.idle":"2025-07-31T20:32:02.213591Z","shell.execute_reply.started":"2025-07-31T20:32:02.183000Z","shell.execute_reply":"2025-07-31T20:32:02.212385Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"from pyspark.sql.functions import to_date,day,month,year,dayofweek\n\ndf = df.withColumn(\"Date\", to_date(\"Date\", \"yyyy-MM-dd\"))\n\ndf=df.withColumn('Day',day(col('Date')))\\\n     .withColumn('Month',month(col('Date')))\\\n      .withColumn('year',year(col('Date')))\\\n       .withColumn('Day_of_week',dayofweek(col('Date')))\ndf=df.drop('Date')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:32:02.214706Z","iopub.execute_input":"2025-07-31T20:32:02.215039Z","iopub.status.idle":"2025-07-31T20:32:02.289355Z","shell.execute_reply.started":"2025-07-31T20:32:02.215008Z","shell.execute_reply":"2025-07-31T20:32:02.288251Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"from pyspark.sql.functions import when\n\ndf = df.withColumn(\"PLU_4046_Ratio\", col(\"PLU_4046\") / col(\"Total Volume\"))\\\n       .withColumn(\"PLU_4225_Ratio\", col(\"PLU_4225\") / col(\"Total Volume\"))\\\n       .withColumn(\"PLU_4770_Ratio\", col(\"PLU_4770\") / col(\"Total Volume\"))\\\n       .withColumn(\"SmallBag_Ratio\", col(\"Small Bags\") / col(\"Total Bags\"))\\\n       .withColumn(\"LargeBag_Ratio\", col(\"Large Bags\") / col(\"Total Bags\"))\\\n       .withColumn(\"XLargeBag_Ratio\", col(\"XLarge Bags\") / col(\"Total Bags\"))\n\ndf = df.withColumn(\"Season\",\n    when(col(\"Month\").isin(12, 1, 2), \"Winter\")\n    .when(col(\"Month\").isin(3, 4, 5), \"Spring\")\n    .when(col(\"Month\").isin(6, 7, 8), \"Summer\")\n    .otherwise(\"Fall\")\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:32:02.290438Z","iopub.execute_input":"2025-07-31T20:32:02.290759Z","iopub.status.idle":"2025-07-31T20:32:02.429353Z","shell.execute_reply.started":"2025-07-31T20:32:02.290730Z","shell.execute_reply":"2025-07-31T20:32:02.427846Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler, MinMaxScaler, StringIndexer\nfrom pyspark.ml.regression import RandomForestRegressor\ndf=df.dropna()\nnumeric_cols = [\n    'Total Volume', 'PLU_4046', 'PLU_4225', 'PLU_4770',\n    'Total Bags', 'Small Bags', 'Large Bags', 'XLarge Bags',\n    'year', 'Day', 'Month', 'Day_of_week',\n    'PLU_4046_Ratio', 'PLU_4225_Ratio', 'PLU_4770_Ratio',\n    'SmallBag_Ratio', 'LargeBag_Ratio', 'XLargeBag_Ratio']\n\ncategorical_cols = ['type', 'region', 'Season']\n\nindexers = [StringIndexer(inputCol=col, outputCol=col + '_encoded') for col in categorical_cols]\nencoded_cols = [col + \"_encoded\" for col in categorical_cols]\n\nassembler_input = numeric_cols + encoded_cols\n\nassembler = VectorAssembler(inputCols=assembler_input, outputCol='assembled_features')\n\nscaler = MinMaxScaler(inputCol='assembled_features', outputCol='features')\n\ntrain_data, test_data = df.randomSplit([0.8, 0.2], seed=1)\n\nrf = RandomForestRegressor(featuresCol='features', labelCol='AveragePrice', numTrees=50, maxDepth=10)\n\npipeline = Pipeline(stages=indexers + [assembler, scaler, rf])\n\nmodel = pipeline.fit(train_data)\n\npredictions = model.transform(test_data)\n\npredictions.select('AveragePrice', 'prediction').show(10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:32:02.430355Z","iopub.execute_input":"2025-07-31T20:32:02.430711Z","iopub.status.idle":"2025-07-31T20:32:22.022587Z","shell.execute_reply.started":"2025-07-31T20:32:02.430682Z","shell.execute_reply":"2025-07-31T20:32:22.021410Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"25/07/31 20:32:09 WARN DAGScheduler: Broadcasting large task binary with size 1132.8 KiB\n25/07/31 20:32:11 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n25/07/31 20:32:13 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n25/07/31 20:32:14 WARN DAGScheduler: Broadcasting large task binary with size 1066.2 KiB\n25/07/31 20:32:16 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n25/07/31 20:32:18 WARN DAGScheduler: Broadcasting large task binary with size 1809.8 KiB\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+------------+------------------+\n|AveragePrice|        prediction|\n+------------+------------------+\n|        0.49|0.7266116534072552|\n|        0.53|0.7046132013173456|\n|        0.53|0.6025084282093021|\n|        0.53|0.6325070658803155|\n|        0.53|0.7738557340867506|\n|        0.56| 0.904388228252231|\n|        0.56| 0.722211679047507|\n|        0.56|0.7012874916133255|\n|        0.57|0.7500310694025011|\n|        0.57| 0.605569264428526|\n+------------+------------------+\nonly showing top 10 rows\n\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"from pyspark.ml.regression import RandomForestRegressor\n\nrmse_rf = evaluator_rmse.evaluate(predictions_rf)\nmae_rf = evaluator_mae.evaluate(predictions_rf)\nr2_rf = evaluator_r2.evaluate(predictions_rf)\n\nprint(f\"RF RMSE: {rmse_rf:.4f}\")\nprint(f\"RF MAE: {mae_rf:.4f}\")\nprint(f\"RF R2: {r2_rf:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:32:22.023967Z","iopub.execute_input":"2025-07-31T20:32:22.024396Z","iopub.status.idle":"2025-07-31T20:32:23.727603Z","shell.execute_reply.started":"2025-07-31T20:32:22.024362Z","shell.execute_reply":"2025-07-31T20:32:23.726646Z"}},"outputs":[{"name":"stdout","text":"RF RMSE: 0.1715\nRF MAE: 0.1273\nRF R2: 0.8218\n","output_type":"stream"}],"execution_count":54}]}